{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f90dda33-15c6-4552-8caf-8f18279f427f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./config $reset_all_data=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8419244-be19-480e-a178-92efc1b55026",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dbutils.widgets.text(\"reset_all_data\", \"false\", \"Reset Data\")\n",
    "reset_all_data = dbutils.widgets.get(\"reset_all_data\") == \"true\"\n",
    "if reset_all_data:\n",
    "    print(f'clearing up db {dbName}')\n",
    "    spark.sql(f\"DROP DATABASE IF EXISTS `{dbName}` CASCADE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fc128f3-12f1-4dfa-8d1e-f0e124b74c4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def pprint(obj):\n",
    "  import pprint\n",
    "  pprint.pprint(obj, compact=True, indent=1, width=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69e953ef-eed7-43da-ab27-471f362f29aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Temp workaround to test if a table exists in shared cluster mode in DBR 14.2 (see SASP-2467)\n",
    "def table_exists(table_name):\n",
    "    try:\n",
    "        spark.table(table_name).isEmpty()\n",
    "    except:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0392881a-8e38-4cde-8bf2-0379d3db4db3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE CATALOG `onlytrends`\nusing catalog.database `onlytrends`.`onlytrends_db`\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def use_and_create_db(catalog, dbName, cloud_storage_path = None):\n",
    "  print(f\"USE CATALOG `{catalog}`\")\n",
    "  spark.sql(f\"USE CATALOG `{catalog}`\")\n",
    "  spark.sql(f\"\"\"create database if not exists `{dbName}` \"\"\")\n",
    "\n",
    "assert catalog not in ['hive_metastore', 'spark_catalog']\n",
    "#If the catalog is defined, we force it to the given value and throw exception if not.\n",
    "if len(catalog) > 0:\n",
    "  current_catalog = spark.sql(\"select current_catalog()\").collect()[0]['current_catalog()']\n",
    "  if current_catalog != catalog:\n",
    "    catalogs = [r['catalog'] for r in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "    if catalog not in catalogs:\n",
    "      bucket_name = \"'s3://databricks-workspace-stack-34ba9-bucket/unity-catalog/4349323632946073'\"\n",
    "      spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog} MANAGED LOCATION {bucket_name}\")\n",
    "      if catalog == 'dbdemos':\n",
    "        spark.sql(f\"ALTER CATALOG {catalog} OWNER TO `account users`\")\n",
    "  use_and_create_db(catalog, dbName)\n",
    "\n",
    "if catalog == 'dbdemos':\n",
    "  try:\n",
    "    spark.sql(f\"GRANT CREATE, USAGE on DATABASE {catalog}.{dbName} TO `account users`\")\n",
    "    spark.sql(f\"ALTER SCHEMA {catalog}.{dbName} OWNER TO `account users`\")\n",
    "  except Exception as e:\n",
    "    print(\"Couldn't grant access to the schema to all users:\"+str(e))    \n",
    "\n",
    "print(f\"using catalog.database `{catalog}`.`{dbName}`\")\n",
    "spark.sql(f\"\"\"USE `{catalog}`.`{dbName}`\"\"\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccc7a633-8cce-419d-9578-2ede95409028",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def upload_pdf_to_volume(url, destination):\n",
    "    local_filename = url.split('/')[-1]\n",
    "    # NOTE the stream=True parameter below\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        print('saving '+destination+'/'+local_filename)\n",
    "        with open(destination+'/'+local_filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                # If you have chunk encoded response uncomment if\n",
    "                # and set chunk_size parameter to None.\n",
    "                #if chunk: \n",
    "                f.write(chunk)\n",
    "    return local_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53f482d9-fe7e-4edb-a701-2bf15e4d4933",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#install poppler on the cluster (should be done by init scripts)\n",
    "def install_ocr_on_nodes():\n",
    "    \"\"\"\n",
    "    install poppler on the cluster (should be done by init scripts)\n",
    "    \"\"\"\n",
    "    # from pyspark.sql import SparkSession\n",
    "    import subprocess\n",
    "    num_workers = max(1,int(spark.conf.get(\"spark.databricks.clusterUsageTags.clusterWorkers\")))\n",
    "    command = \"sudo rm -rf /var/cache/apt/archives/* /var/lib/apt/lists/* && sudo apt-get purge && sudo apt-get clean && sudo apt-get update && sudo apt-get install poppler-utils tesseract-ocr -y\" \n",
    "    def run_subprocess(command):\n",
    "        try:\n",
    "            output = subprocess.check_output(command, stderr=subprocess.STDOUT, shell=True)\n",
    "            return output.decode()\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            raise Exception(\"An error occurred installing OCR libs:\"+ e.output.decode())\n",
    "    #install on the driver\n",
    "    run_subprocess(command)\n",
    "    def run_command(iterator):\n",
    "        for x in iterator:\n",
    "            yield run_subprocess(command)\n",
    "    # spark = SparkSession.builder.getOrCreate()\n",
    "    data = spark.sparkContext.parallelize(range(num_workers), num_workers) \n",
    "    # Use mapPartitions to run command in each partition (worker)\n",
    "    output = data.mapPartitions(run_command)\n",
    "    try:\n",
    "        output.collect();\n",
    "        print(\"OCR libraries installed\")\n",
    "    except Exception as e:\n",
    "        print(f\"Couldn't install on all node: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b181e9d-932f-4117-9213-0937f9426be3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def endpoint_exists(vsc, vs_endpoint_name):\n",
    "  try:\n",
    "    return vs_endpoint_name in [e['name'] for e in vsc.list_endpoints().get('endpoints', [])]\n",
    "  except Exception as e:\n",
    "    #Temp fix for potential REQUEST_LIMIT_EXCEEDED issue\n",
    "    if \"REQUEST_LIMIT_EXCEEDED\" in str(e):\n",
    "      print(\"WARN: couldn't get endpoint status due to REQUEST_LIMIT_EXCEEDED error. The demo will consider it exists\")\n",
    "      return True\n",
    "    else:\n",
    "      raise e\n",
    "\n",
    "def wait_for_vs_endpoint_to_be_ready(vsc, vs_endpoint_name):\n",
    "  for i in range(180):\n",
    "    try:\n",
    "      endpoint = vsc.get_endpoint(vs_endpoint_name)\n",
    "    except Exception as e:\n",
    "      #Temp fix for potential REQUEST_LIMIT_EXCEEDED issue\n",
    "      if \"REQUEST_LIMIT_EXCEEDED\" in str(e):\n",
    "        print(\"WARN: couldn't get endpoint status due to REQUEST_LIMIT_EXCEEDED error. Please manually check your endpoint status\")\n",
    "        return\n",
    "      else:\n",
    "        raise e\n",
    "    status = endpoint.get(\"endpoint_status\", endpoint.get(\"status\"))[\"state\"].upper()\n",
    "    if \"ONLINE\" in status:\n",
    "      return endpoint\n",
    "    elif \"PROVISIONING\" in status or i <6:\n",
    "      if i % 20 == 0: \n",
    "        print(f\"Waiting for endpoint to be ready, this can take a few min... {endpoint}\")\n",
    "      time.sleep(10)\n",
    "    else:\n",
    "      raise Exception(f'''Error with the endpoint {vs_endpoint_name}. - this shouldn't happen: {endpoint}.\\n Please delete it and re-run the previous cell: vsc.delete_endpoint(\"{vs_endpoint_name}\")''')\n",
    "  raise Exception(f\"Timeout, your endpoint isn't ready yet: {vsc.get_endpoint(vs_endpoint_name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bffa2d64-e0b4-4fca-80dc-4e75f3e523fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def index_exists(vsc, endpoint_name, index_full_name):\n",
    "    try:\n",
    "        vsc.get_index(endpoint_name, index_full_name).describe()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        if 'RESOURCE_DOES_NOT_EXIST' not in str(e):\n",
    "            print(f'Unexpected error describing the index. This could be a permission issue.')\n",
    "            raise e\n",
    "    return False\n",
    "\n",
    "def wait_for_index_to_be_ready(vsc, vs_endpoint_name, index_name):\n",
    "  for i in range(180):\n",
    "    idx = vsc.get_index(vs_endpoint_name, index_name).describe()\n",
    "    index_status = idx.get('status', idx.get('index_status', {}))\n",
    "    status = index_status.get('detailed_state', index_status.get('status', 'UNKNOWN')).upper()\n",
    "    url = index_status.get('index_url', index_status.get('url', 'UNKNOWN'))\n",
    "    if \"ONLINE\" in status:\n",
    "      return\n",
    "    if \"UNKNOWN\" in status:\n",
    "      print(f\"Can't get the status - will assume index is ready {idx} - url: {url}\")\n",
    "      return\n",
    "    elif \"PROVISIONING\" in status:\n",
    "      if i % 40 == 0: print(f\"Waiting for index to be ready, this can take a few min... {index_status} - pipeline url:{url}\")\n",
    "      time.sleep(10)\n",
    "    else:\n",
    "        raise Exception(f'''Error with the index - this shouldn't happen. DLT pipeline might have been killed.\\n Please delete it and re-run the previous cell: vsc.delete_index(\"{index_name}, {vs_endpoint_name}\") \\nIndex details: {idx}''')\n",
    "  raise Exception(f\"Timeout, your index isn't ready yet: {vsc.get_index(index_name, vs_endpoint_name)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6202241f-4a18-4498-a825-afd7d31cf0b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def test_demo_permissions(host, secret_scope, secret_key, vs_endpoint_name, index_name, embedding_endpoint_name = None, managed_embeddings = True):\n",
    "  error = False\n",
    "  CSS_REPORT = \"\"\"\n",
    "  <style>\n",
    "  .dbdemos_install{\n",
    "                      font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,Noto Sans,sans-serif,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji,FontAwesome;\n",
    "  color: #3b3b3b;\n",
    "  box-shadow: 0 .15rem 1.15rem 0 rgba(58,59,69,.15)!important;\n",
    "  padding: 10px 20px 20px 20px;\n",
    "  margin: 10px;\n",
    "  font-size: 14px !important;\n",
    "  }\n",
    "  .dbdemos_block{\n",
    "      display: block !important;\n",
    "      width: 900px;\n",
    "  }\n",
    "  .code {\n",
    "      padding: 5px;\n",
    "      border: 1px solid #e4e4e4;\n",
    "      font-family: monospace;\n",
    "      background-color: #f5f5f5;\n",
    "      margin: 5px 0px 0px 0px;\n",
    "      display: inline;\n",
    "  }\n",
    "  </style>\"\"\"\n",
    "\n",
    "  def display_error(title, error, color=\"\"):\n",
    "    displayHTML(f\"\"\"{CSS_REPORT}\n",
    "      <div class=\"dbdemos_install\">\n",
    "                          <h1 style=\"color: #eb0707\">Configuration error: {title}</h1> \n",
    "                            {error}\n",
    "                        </div>\"\"\")\n",
    "  \n",
    "  def get_email():\n",
    "    try:\n",
    "      return spark.sql('select current_user() as user').collect()[0]['user']\n",
    "    except:\n",
    "      return 'Uknown'\n",
    "\n",
    "  def get_token_error(msg, e):\n",
    "    return f\"\"\"\n",
    "    {msg}<br/><br/>\n",
    "    Your model will be served using Databrick Serverless endpoint and needs a Pat Token to authenticate.<br/>\n",
    "    <strong> This must be saved as a secret to be accessible when the model is deployed.</strong><br/><br/>\n",
    "    Here is how you can add the Pat Token as a secret available within your notebook and for the model:\n",
    "    <ul>\n",
    "    <li>\n",
    "      first, setup the Databricks CLI on your laptop or using this cluster terminal:\n",
    "      <div class=\"code dbdemos_block\">pip install databricks-cli</div>\n",
    "    </li>\n",
    "    <li> \n",
    "      Configure the CLI. You'll need your workspace URL and a PAT token from your profile page\n",
    "      <div class=\"code dbdemos_block\">databricks configure</div>\n",
    "    </li>  \n",
    "    <li>\n",
    "      Create the dbdemos scope:\n",
    "      <div class=\"code dbdemos_block\">databricks secrets create-scope dbdemos</div>\n",
    "    <li>\n",
    "      Save your service principal secret. It will be used by the Model Endpoint to autenticate. <br/>\n",
    "      If this is a demo/test, you can use one of your PAT token.\n",
    "      <div class=\"code dbdemos_block\">databricks secrets put-secret dbdemos rag_sp_token</div>\n",
    "    </li>\n",
    "    <li>\n",
    "      Optional - if someone else created the scope, make sure they give you read access to the secret:\n",
    "      <div class=\"code dbdemos_block\">databricks secrets put-acl dbdemos '{get_email()}' READ</div>\n",
    "\n",
    "    </li>  \n",
    "    </ul>  \n",
    "    <br/>\n",
    "    Detailed error trying to access the secret:\n",
    "      <div class=\"code dbdemos_block\">{e}</div>\"\"\"\n",
    "\n",
    "  try:\n",
    "    secret = dbutils.secrets.get(secret_scope, secret_key)\n",
    "    secret_principal = \"__UNKNOWN__\"\n",
    "    try:\n",
    "      from databricks.sdk import WorkspaceClient\n",
    "      w = WorkspaceClient(token=dbutils.secrets.get(secret_scope, secret_key), host=host)\n",
    "      secret_principal = w.current_user.me().emails[0].value\n",
    "    except Exception as e_sp:\n",
    "      error = True\n",
    "      display_error(f\"Couldn't get the SP identity using the Pat Token saved in your secret\", \n",
    "                    get_token_error(f\"<strong>This likely means that the Pat Token saved in your secret {secret_scope}/{secret_key} is incorrect or expired. Consider replacing it.</strong>\", e_sp))\n",
    "      return\n",
    "  except Exception as e:\n",
    "    error = True\n",
    "    display_error(f\"We couldn't access the Pat Token saved in the secret {secret_scope}/{secret_key}\", \n",
    "                  get_token_error(\"<strong>This likely means your secret isn't set or not accessible for your user</strong>.\", e))\n",
    "    return\n",
    "  \n",
    "  try:\n",
    "    from databricks.vector_search.client import VectorSearchClient\n",
    "    vsc = VectorSearchClient(workspace_url=host, personal_access_token=secret, disable_notice=True)\n",
    "    vs_index = vsc.get_index(endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME, index_name=index_name)\n",
    "    if embedding_endpoint_name:\n",
    "      if managed_embeddings:\n",
    "        from langchain_community.embeddings import DatabricksEmbeddings\n",
    "        results = vs_index.similarity_search(query_text='What is Apache Spark?', columns=[\"content\"], num_results=1)\n",
    "      else:\n",
    "        from langchain_community.embeddings import DatabricksEmbeddings\n",
    "        embedding_model = DatabricksEmbeddings(endpoint=embedding_endpoint_name)\n",
    "        embeddings = embedding_model.embed_query('What is Apache Spark?')\n",
    "        results = vs_index.similarity_search(query_vector=embeddings, columns=[\"content\"], num_results=1)\n",
    "\n",
    "  except Exception as e:\n",
    "    error = True\n",
    "    vs_error = f\"\"\"\n",
    "    Why are we getting this error?<br/>\n",
    "    The model is using the Pat Token saved with the secret {secret_scope}/{secret_key} to access your vector search index '{index_name}' (host:{host}).<br/><br/>\n",
    "    To do so, the principal owning the Pat Token must have USAGE permission on your schema and READ permission on the index.<br/>\n",
    "    The principal is the one who generated the token you saved as secret: `{secret_principal}`. <br/>\n",
    "    <i>Note: Production-grade deployement should to use a Service Principal ID instead.</i><br/>\n",
    "    <br/>\n",
    "    Here is how you can fix it:<br/><br/>\n",
    "    <strong>Make sure your Service Principal has USE privileve on the schema</strong>:\n",
    "    <div class=\"code dbdemos_block\">\n",
    "    spark.sql('GRANT USAGE ON CATALOG `{catalog}` TO `{secret_principal}`');<br/>\n",
    "    spark.sql('GRANT USAGE ON DATABASE `{catalog}`.`{db}` TO `{secret_principal}`');<br/>\n",
    "    </div>\n",
    "    <br/>\n",
    "    <strong>Grant SELECT access to your SP to your index:</strong>\n",
    "    <div class=\"code dbdemos_block\">\n",
    "    from databricks.sdk import WorkspaceClient<br/>\n",
    "    import databricks.sdk.service.catalog as c<br/>\n",
    "    WorkspaceClient().grants.update(c.SecurableType.TABLE, \"{index_name}\",<br/>\n",
    "                                            changes=[c.PermissionsChange(add=[c.Privilege[\"SELECT\"]], principal=\"{secret_principal}\")])\n",
    "    </div>\n",
    "    <br/>\n",
    "    <strong>If this is still not working, make sure the value saved in your {secret_scope}/{secret_key} secret is your SP pat token </strong>.<br/>\n",
    "    <i>Note: if you're using a shared demo workspace, please do not change the secret value if was set to a valid SP value by your admins.</i>\n",
    "\n",
    "    <br/>\n",
    "    <br/>\n",
    "    Detailed error trying to access the endpoint:\n",
    "    <div class=\"code dbdemos_block\">{str(e)}</div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    if \"403\" in str(e):\n",
    "      display_error(f\"Permission error on Vector Search index {index_name} using the endpoint {vs_endpoint_name} and secret {secret_scope}/{secret_key}\", vs_error)\n",
    "    else:\n",
    "      display_error(f\"Unkown error accessing the Vector Search index {index_name} using the endpoint {vs_endpoint_name} and secret {secret_scope}/{secret_key}\", vs_error)\n",
    "  def get_wid():\n",
    "    try:\n",
    "      return dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('orgId')\n",
    "    except:\n",
    "      return None\n",
    "  if get_wid() in [\"5206439413157315\", \"984752964297111\", \"1444828305810485\", \"2556758628403379\"]:\n",
    "    print(f\"----------------------------\\nYou are in a Shared FE workspace. Please don't override the secret value (it's set to the SP `{secret_principal}`).\\n---------------------------\")\n",
    "\n",
    "  if not error:\n",
    "    print('Secret and permissions seems to be properly setup, you can continue the demo!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "348ce562-83bc-42e1-9111-5d42a8858f04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def display_chat(chat_history, response):\n",
    "  def user_message_html(message):\n",
    "    return f\"\"\"\n",
    "      <div style=\"width: 90%; border-radius: 10px; background-color: #c2efff; padding: 10px; box-shadow: 2px 2px 2px #F7f7f7; margin-bottom: 10px; font-size: 14px;\">\n",
    "        {message}\n",
    "      </div>\"\"\"\n",
    "  def assistant_message_html(message):\n",
    "    return f\"\"\"\n",
    "      <div style=\"width: 90%; border-radius: 10px; background-color: #e3f6fc; padding: 10px; box-shadow: 2px 2px 2px #F7f7f7; margin-bottom: 10px; margin-left: 40px; font-size: 14px\">\n",
    "        <img style=\"float: left; width:40px; margin: -10px 5px 0px -10px\" src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/chatbot-rag/robot.png?raw=true\"/>\n",
    "        {message}\n",
    "      </div>\"\"\"\n",
    "  chat_history_html = \"\".join([user_message_html(m[\"content\"]) if m[\"role\"] == \"user\" else assistant_message_html(m[\"content\"]) for m in chat_history])\n",
    "  answer = response[\"result\"].replace('\\n', '<br/>')\n",
    "  sources_html = (\"<br/><br/><br/><strong>Sources:</strong><br/> <ul>\" + '\\n'.join([f\"\"\"<li><a href=\"{s}\">{s}</a></li>\"\"\" for s in response[\"sources\"]]) + \"</ul>\") if response[\"sources\"] else \"\"\n",
    "  response_html = f\"\"\"{answer}{sources_html}\"\"\"\n",
    "\n",
    "  displayHTML(chat_history_html + assistant_message_html(response_html))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "init",
   "widgets": {
    "reset_all_data": {
     "currentValue": "false",
     "nuid": "2df87e6c-609c-49aa-b78c-e973c6e918b2",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "false",
      "label": "Reset Data",
      "name": "reset_all_data",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
